{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/Users/mac/Desktop/PycharmProjects/TAADL/src')\n",
    "sys.path.insert(2, '/Users/mac/Desktop/PycharmProjects/TAADL/models')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from config import DATA_PATH\n",
    "from typing import Optional, Tuple\n",
    "from scipy.stats import norm\n",
    "from torch.linalg import det, inv\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "\n",
    "\n",
    "prices   = pd.read_csv(DATA_PATH+'/prices.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform(Z:torch.Tensor, context_len:Optional[int]=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    transforms data distribution to standard normal distribution. \n",
    "    \n",
    "    Transform takes 2 steps:\n",
    "    1. estimate empirical distribution of the data per asset. Then convert it to uniform, u [0,1] distribution\n",
    "        note that we use step function based empirical CDF which is different to the one specified in the original paper.\n",
    "        ** values are truncated to prevent standard normal variable goes either -inf or inf.\n",
    "\n",
    "    2. use inverse CDF of standard normal to convert u to x, where x follows standard normal distribution.\n",
    "    \n",
    "      (1)  (2)\n",
    "    z -> u -> x \n",
    "\n",
    "    INPUTS\n",
    "        df: pd.DataFrame\n",
    "            input data sequence, that is multivariate\n",
    "        context_len: Optional[int]\n",
    "            specifies context length for the training set. rest of the data will be used for prediction.\n",
    "    \n",
    "    RETURNS\n",
    "        X: pd.DataFrame\n",
    "            returns transformed dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    m = 100 if context_len is None else context_len\n",
    "    \n",
    "    # lower and upper bound for emp. CDF\n",
    "    delta_m = (4 * np.sqrt(np.log(m) * np.pi) * m ** 0.25) ** -1 \n",
    "\n",
    "    for i in range(Z_test.size(1)):\n",
    "        Z_i = Z[:,i].numpy()\n",
    "        \n",
    "        # estimate empirical CDF\n",
    "        # only use m datapoint to estimate CDF.\n",
    "        emp_cdf = ECDF(Z_i[:m].reshape(-1))\n",
    "        \n",
    "        # for each datapoint, transform\n",
    "        U_i = emp_cdf(Z_i)\n",
    "        \n",
    "        # truncate extreme values\n",
    "        U_i = np.where(U_i < delta_m, delta_m, U_i) \n",
    "        U_i = np.where(U_i > 1-delta_m, 1-delta_m, U_i)\n",
    "        \n",
    "        # get standard normal values\n",
    "        X_i = norm.ppf(q=U_i, loc=0, scale=1) # inverse CDF of standard normal\n",
    "        X.append(torch.Tensor(X_i))\n",
    "        \n",
    "    # make a dataframe\n",
    "    X = torch.stack(X, axis=0).T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVariateGP(nn.Module):\n",
    "    def __init__(self, input_size:int, rank_size:int):\n",
    "        super(MultiVariateGP,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.rank_size  = rank_size\n",
    "\n",
    "        # define linear weight for calculating parameters of gaussian process\n",
    "        # these weights are SHARED across all time series.\n",
    "        self.layer_m = nn.Linear(input_size, 1) # mean \n",
    "        self.layer_d = nn.Linear(input_size, rank_size) # diagonal point\n",
    "        self.layer_v = nn.Sequential(\n",
    "                                        nn.Linear(input_size, 1),\n",
    "                                        nn.Softplus(beta=1)\n",
    "                                    ) # volatility\n",
    "\n",
    "    def forward(self, h_t:torch.Tensor):\n",
    "        m_t = self.layer_m(h_t)\n",
    "        d_t = self.layer_d(h_t)\n",
    "        v_t = self.layer_v(h_t)\n",
    "        return m_t, d_t, v_t\n",
    "\n",
    "\n",
    "class GPAutoregressiveRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size:int, \n",
    "        hidden_size:int, \n",
    "        num_layers:int, \n",
    "        batch_size:int,\n",
    "        rank_size:int,\n",
    "        dropout:float=0.0,\n",
    "        batch_first:bool=False,\n",
    "        features:Optional[dict]=None \n",
    "    ):\n",
    "        super(GPAutoregressiveRNN,self).__init__()\n",
    "        self.input_size   = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.batch_size   = batch_size\n",
    "        self.num_layers   = num_layers\n",
    "        self.batch_first  = batch_first\n",
    "        self.feature_size = len(features[0]) if features is not None else 0\n",
    "        self.features = features\n",
    "        self.hidden   = None\n",
    "        \n",
    "        # local lstm\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                             num_layers=num_layers, batch_first=batch_first, dropout=dropout)\n",
    "        \n",
    "        # Multivariate Gaussian Process\n",
    "        self.gp = MultiVariateGP(input_size=hidden_size + self.feature_size, rank_size=rank_size)\n",
    "\n",
    "    def init_weight(self) -> None:\n",
    "        h0, c0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\\\n",
    "                    torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
    "        self.hidden = (h0, c0)\n",
    "\n",
    "    def feature_selector(self, indices:torch.Tensor, time_steps:int):\n",
    "        feature = []\n",
    "        for idx in indices:\n",
    "            feature.append(self.e_dict[idx.item()].repeat(time_steps,1))\n",
    "        feature = torch.stack(feature, axis=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, z_t:torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # we only use the subset of batch for training\n",
    "        batch_indices = torch.randperm(7)[:self.batch_size]\n",
    "\n",
    "        # select the batch then pass it through the unrolled LSTM\n",
    "        z_t = z_t[:,batch_indices].unsqueeze(2)# timestep x num_batch x 1\n",
    "        output, self.hidden = self.lstm(z_t, self.hidden)\n",
    "\n",
    "        # find feature vector e_i for each asset i then concatenate it with LSTM output.\n",
    "        e = self.feature_selector(batch_indices, z_t.size(0))\n",
    "        y_t = torch.concat([output, e], axis=2)\n",
    "\n",
    "        # get GP parameters\n",
    "        # calculate parameters of multivariate gaussian process for each time t\n",
    "        mu_t, v_t, d_t = deepar.gp(y_t)\n",
    "        cov_t = torch.diag_embed(d_t.squeeze(2)) + (v_t @ v_t.permute(0,2,1)) # D + V @ V.T\n",
    "        return mu_t, cov_t, batch_indices\n",
    "\n",
    "    def predict(self, z_t:torch.Tensor, hidden_state=None) -> torch.Tensor:\n",
    "        assert z_t.size(1) == 1, 'input is a sequence! input should include only one time step.'\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        linear = nn.Linear(4,1)\n",
    "\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.hidden\n",
    "    \n",
    "        for t in range(self.prediction_len):\n",
    "            h_t, hidden_state = self.lstm(z_t, hidden_state)\n",
    "            z_t = linear(h_t)\n",
    "            outputs.append(z_t)\n",
    "            hidden_states.append(h_t)        \n",
    "        return torch.stack(outputs, axis=2), torch.stack(hidden_states, axis=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess weekly data\n",
    "prices.index = pd.to_datetime(prices.index)\n",
    "prices = prices.resample('W').last()\n",
    "\n",
    "lret_w = np.log(prices/prices.shift(1)).fillna(0.0) # weekly return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature vector, which is speical to each time series\n",
    "# first 5: stock, gov bond, corp bond, real estate, commodity\n",
    "# last 3 : us, europe, other\n",
    "\n",
    "e1 = torch.Tensor([1,0,0,0,0,0,1,0]) # IEV, europe\n",
    "e2 = torch.Tensor([1,0,0,0,0,0,0,1]) # EEM, em. market\n",
    "e3 = torch.Tensor([0,0,1,0,0,1,0,0]) # AGG, us corp bonds\n",
    "e4 = torch.Tensor([0,1,0,0,0,1,0,0]) # IEF, us gov bonds\n",
    "e5 = torch.Tensor([0,0,0,1,0,1,0,0]) # IYR, us real estates\n",
    "e6 = torch.Tensor([0,0,0,0,1,0,0,0]) # IAU, gold\n",
    "e7 = torch.Tensor([1,0,0,0,0,1,0,0]) # ITOT, us equities\n",
    "\n",
    "e_dict = {0: e1, 1: e2, 2: e3, 3: e4, 4: e5, 5: e6, 6: e7}\n",
    "\n",
    "# generate random sample\n",
    "Z_test = torch.Tensor(np.random.normal(0.0, 0.05, (200,7)))\n",
    "X_test = transform(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar = GPAutoregressiveRNN(input_size=1, hidden_size=4, num_layers=2, rank_size=2, batch_size=3, features=e_dict)\n",
    "deepar.init_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hidden states of the input by unrolled LSTM\n",
    "mu_t, cov_t, idx = deepar(Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = torch.distributions.multivariate_normal.MultivariateNormal(mu_t[0].view(-1), cov_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.8473, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg.log_prob(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianLL_loss(x:torch.Tensor, mu_t:torch.Tensor, cov_t:torch.Tensor):\n",
    "    assert len(cov_t.size()) == 3 or len(mu_t.size()) == 3, \\\n",
    "        'one of inputs dimension is not equal to 3.'\n",
    "    \n",
    "    d = cov_t.size(1) # dimension\n",
    "    if len(x.size()) == 2:\n",
    "        x = x.unsqueeze(2)\n",
    "\n",
    "    ll = (x - mu_t).permute(0,2,1) @ inv(cov_t) @ (x - mu_t) # (x - mu).T @ inverse cov matrix @ (x - mu)\n",
    "    ll = -0.5 * (d * np.log(2 * torch.pi) + det(cov_t).unsqueeze(1) + ll.squeeze(2)) # ll + det(cov)\n",
    "    return -ll.sum() # negative log-likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6921, -1.0946,  0.9929])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fcdd98ffdc43e19459281bdd6dd2d99dc416cd8bc0df76bf27c994662bebc1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
