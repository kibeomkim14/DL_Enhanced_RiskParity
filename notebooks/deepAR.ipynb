{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/Users/mac/Desktop/PycharmProjects/TAADL/src')\n",
    "sys.path.insert(2, '/Users/mac/Desktop/PycharmProjects/TAADL/models')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from config import DATA_PATH\n",
    "from typing import Optional, Tuple\n",
    "from scipy.stats import norm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "class ECDF_(object):\n",
    "    def __init__(self, data:np.ndarray):\n",
    "        self.x = data\n",
    "        self.x.sort()\n",
    "        self.x_min, self.x_max = self.x[0], self.x[-1]\n",
    "        self.n = len(self.x)\n",
    "        self.y = np.linspace(1.0/self.n, 1.0, self.n)\n",
    "        self.f = interp1d(self.x, self.y, fill_value='extrapolate') # make interpolation\n",
    "        self.inv_f = interp1d(self.y, self.x, fill_value='extrapolate') # inverse is just arguments reversed\n",
    "        \n",
    "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        calculates y given x under defined ECDF class.\n",
    "        \"\"\"\n",
    "        if np.sum(x > self.x_max) > 0 or np.sum(x < self.x_min) > 0:\n",
    "            x = np.where(x > self.x_max, self.x_max, x)\n",
    "            x = np.where(x < self.x_min, self.x_min, x)\n",
    "        return self.f(x)\n",
    "\n",
    "    def inverse(self, y:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        calculates the inverse of ECDF with y as an input.\n",
    "        \"\"\"\n",
    "         # if cdf value is less than 0 or more than 1, trim values\n",
    "        if np.sum(y > 1.0) > 0 or np.sum(y < 0.0) > 0:\n",
    "            y = np.where(y > 1.0, 1.0, y)\n",
    "            y = np.where(y < 0.0, 0.0, y)\n",
    "        # otherwise, return\n",
    "        return self.inv_f(y)\n",
    "\n",
    "def gaussian_loss(x:torch.Tensor, mu_t:torch.Tensor, cov_t:torch.Tensor) -> torch.Tensor:\n",
    "    assert x.size(0) == mu_t.size(0), \\\n",
    "        'sequence length is not equal (input and mu_t)'\n",
    "    assert mu_t.size(0) == cov_t.size(0), \\\n",
    "        'sequence length is not equal (mu_t and cov_t)'    \n",
    "    assert len(cov_t.size()) == 3, \\\n",
    "        'dimension of covariance matrix is not equal to 3.'\n",
    "\n",
    "    if len(mu_t.size()) == 3:\n",
    "        mu_t = mu_t.squeeze(2)\n",
    "\n",
    "    # set a mul;tivariate distribution indexed by time t\n",
    "    # loc: batch_size x dim, cov: batch_size x dim x dim\n",
    "    # input x: batch_size x dim\n",
    "    distribution_t = MultivariateNormal(mu_t, cov_t)\n",
    "    loglikelihood = distribution_t.log_prob(x)\n",
    "    return -loglikelihood.sum() # negative log-likelihood\n",
    "\n",
    "\n",
    "def transform(Z:torch.Tensor, cdfs:Optional[dict]=None) -> Tuple[torch.Tensor,dict]:\n",
    "    \"\"\"\n",
    "    transforms data distribution to standard normal distribution. \n",
    "    \n",
    "    Transform takes 2 steps:\n",
    "    1. estimate empirical distribution of the data per asset. Then convert it to uniform, u [0,1] distribution\n",
    "        note that we use step function based empirical CDF which is different to the one specified in the original paper.\n",
    "        ** values are truncated to prevent standard normal variable goes either -inf or inf.\n",
    "\n",
    "    2. use inverse CDF of standard normal to convert u to x, where x follows standard normal distribution.\n",
    "    \n",
    "      (1)  (2)\n",
    "    z -> u -> x \n",
    "\n",
    "    INPUTS\n",
    "        df: pd.DataFrame\n",
    "            input data sequence, that is multivariate\n",
    "        context_len: Optional[int]\n",
    "            specifies context length for the training set. rest of the data will be used for prediction.\n",
    "    \n",
    "    RETURNS\n",
    "        X: pd.DataFrame\n",
    "            returns transformed dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    m = Z.size(0)\n",
    "    emp_distributions = {}\n",
    "    \n",
    "    # lower and upper bound for emp. CDF\n",
    "    delta_m = (4 * np.sqrt(np.log(m) * np.pi) * m ** 0.25) ** -1 \n",
    "\n",
    "    for i in range(Z.size(1)):\n",
    "        Z_i = Z[:,i].numpy()\n",
    "        \n",
    "        # estimate empirical CDF\n",
    "        # only use m datapoint to estimate CDF.\n",
    "        if cdfs is not None:\n",
    "            emp_cdf = cdfs['CDF_'+str(i)]\n",
    "        else:\n",
    "            emp_cdf = ECDF_(Z_i.reshape(-1))\n",
    "        \n",
    "        # for each datapoint, transform\n",
    "        U_i = emp_cdf(Z_i)\n",
    "        \n",
    "        # truncate extreme values\n",
    "        U_i = np.where(U_i < delta_m, delta_m, U_i) \n",
    "        U_i = np.where(U_i > 1-delta_m, 1-delta_m, U_i)\n",
    "        \n",
    "        # get standard normal values\n",
    "        X_i = norm.ppf(q=U_i, loc=0, scale=1) # inverse CDF of standard normal\n",
    "        X.append(torch.Tensor(X_i))\n",
    "        emp_distributions['CDF_'+str(i)] = emp_cdf\n",
    "        \n",
    "    # make a dataframe\n",
    "    X = torch.stack(X, axis=0).T\n",
    "    return X, emp_distributions\n",
    "\n",
    "\n",
    "def inv_transform(X:torch.Tensor, cdfs:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    inverse transforms standard normal distribution to the original distribution given a set of\n",
    "    empirical cdfs.\n",
    "    \n",
    "    Transform takes 2 steps:\n",
    "    1. use standard normal to convert x to u, where u is in [0,1].\n",
    "    2. the input cdfs contains empirical cdfs with its inverse. Using inverse of these cdfs, we transform u to z.\n",
    "    \n",
    "      (1)  (2)\n",
    "    x -> u -> z\n",
    "\n",
    "    INPUTS\n",
    "        X: torch.Tensor\n",
    "            input data sequence, that is multivariate\n",
    "        cdfs: dict\n",
    "            a dictionary of empirical cdfs indexed by asset number.\n",
    "    \n",
    "    RETURNS\n",
    "        Z: torch.Tensor\n",
    "            returns original dataset.\n",
    "    \"\"\" \n",
    "    Z = []\n",
    "\n",
    "    for i in range(X.size(1)):\n",
    "        X_i = X[:,i].numpy()\n",
    "        \n",
    "        # for each datapoint, transform to u by applying standard normal CDF\n",
    "        U_i = norm.cdf(X_i)\n",
    "        \n",
    "        # get empirical distribution of each asset.\n",
    "        emp_cdf = cdfs['CDF_'+str(i)]\n",
    "\n",
    "        # transform the uniform data to Z by inverse empirical CDF\n",
    "        Z_i = emp_cdf.inverse(U_i) \n",
    "        Z.append(torch.Tensor(Z_i))\n",
    "\n",
    "    Z = torch.stack(Z, axis=0).T\n",
    "    return Z\n",
    "\n",
    "def train_idx_sampler(tr_idx:int, context_len:int, prediction_len:int, num_samples:int) -> list:\n",
    "    \"\"\"\n",
    "    Given a traini index (a point), with context length and prediction length, this function samples a sequence of length \n",
    "    context_len + prediction_len for 'num_samples' times.\n",
    "    \n",
    "    INPUTS\n",
    "        tr_idx:int\n",
    "            an end point of training dataset. It is an integer\n",
    "        context_len:int\n",
    "            specifies the length of training interval. \n",
    "        prediction_len:int\n",
    "            specifies the length of prediction interval. \n",
    "        num_samples:\n",
    "            specifies the number of samples to be sampled\n",
    "    \n",
    "    RETURNS\n",
    "        sample_indices: list\n",
    "            a list of sampled indices each in the form of (training indices, prediction indices)\n",
    "    \"\"\"\n",
    "    sample_indices = []\n",
    "    for idx in np.random.randint(0,tr_idx - context_len, size=num_samples):\n",
    "        sample_indices.append((torch.LongTensor(idx + np.arange(context_len)), \\\n",
    "                            torch.LongTensor(idx + np.arange(prediction_len)+ context_len)))\n",
    "    return sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVariateGP(nn.Module):\n",
    "    def __init__(self, input_size:int, rank_size:int):\n",
    "        super(MultiVariateGP,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.rank_size  = rank_size\n",
    "\n",
    "        # define linear weight for calculating parameters of gaussian process\n",
    "        # these weights are SHARED across all time series.\n",
    "        self.layer_m = nn.Linear(input_size, 1) # mean \n",
    "        self.layer_d = nn.Linear(input_size, rank_size) # diagonal point\n",
    "        self.layer_v = nn.Sequential(\n",
    "                                        nn.Linear(input_size, 1),\n",
    "                                        nn.Softplus(beta=1)\n",
    "                                    ) # volatility\n",
    "\n",
    "    def forward(self, h_t:torch.Tensor):\n",
    "        m_t = self.layer_m(h_t)\n",
    "        d_t = self.layer_d(h_t)\n",
    "        v_t = self.layer_v(h_t)\n",
    "        return m_t, d_t, v_t\n",
    "\n",
    "\n",
    "class GPAutoregressiveRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size:int, \n",
    "        hidden_size:int, \n",
    "        num_layers:int, \n",
    "        batch_size:int,\n",
    "        num_asset:int,\n",
    "        rank_size:int,\n",
    "        dropout:float=0.1,\n",
    "        batch_first:bool=False,\n",
    "        features:Optional[dict]=None \n",
    "    ):\n",
    "        super(GPAutoregressiveRNN,self).__init__()\n",
    "        self.input_size   = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.batch_size   = batch_size\n",
    "        self.num_asset    = num_asset\n",
    "        self.num_layers   = num_layers\n",
    "        self.batch_first  = batch_first\n",
    "        self.feature_size = len(features[0]) if features is not None else 0\n",
    "        self.features = features\n",
    "        self.hidden   = None\n",
    "        \n",
    "        # local lstm\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                             num_layers=num_layers, batch_first=batch_first, dropout=dropout)\n",
    "        \n",
    "        # Multivariate Gaussian Process\n",
    "        self.gp = MultiVariateGP(input_size=hidden_size + self.feature_size, rank_size=rank_size)\n",
    "\n",
    "    def init_weight(self) -> None:\n",
    "        h0, c0 = torch.zeros(self.num_layers, self.num_asset, self.hidden_size),\\\n",
    "                    torch.zeros(self.num_layers, self.num_asset, self.hidden_size)\n",
    "        self.hidden = (h0, c0)\n",
    "\n",
    "    def feature_selector(self, indices:torch.Tensor, time_steps:int) -> torch.Tensor:\n",
    "        feature = []\n",
    "        for idx in indices:\n",
    "            feature.append(self.features[idx.item()].repeat(time_steps,1))\n",
    "        feature = torch.stack(feature, axis=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, z_t:torch.Tensor, pred:bool=False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        assert len(z_t.size()) == 2, 'input tensor dimension is not equal to 2.'\n",
    "        z_t = z_t.unsqueeze(2)\n",
    "\n",
    "        # select the batch then pass it through the unrolled LSTM\n",
    "        # timestep x num_batch x 1\n",
    "        output, (hn, cn) = self.lstm(z_t, self.hidden)\n",
    "        self.hidden = (hn.detach(), cn.detach())\n",
    "        \n",
    "        # we only use the subset of batch for training\n",
    "        # else return all indices\n",
    "        if pred:\n",
    "            batch_indices = torch.arange(self.num_asset)\n",
    "        else:\n",
    "            batch_indices = torch.randperm(self.num_asset)[:self.batch_size]\n",
    "        \n",
    "        # find feature vector e_i for each asset i then concatenate it with LSTM output.\n",
    "        e = self.feature_selector(batch_indices, z_t.size(0))\n",
    "        y_t = torch.concat([output[:,batch_indices,:], e], axis=2)\n",
    "\n",
    "        # get GP parameters\n",
    "        # calculate parameters of multivariate gaussian process for each time t\n",
    "        mu_t, v_t, d_t = self.gp(y_t)\n",
    "        cov_t = torch.diag_embed(d_t.squeeze(2)) + (v_t @ v_t.permute(0,2,1)) # D + V @ V.T\n",
    "        return mu_t, cov_t, batch_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(DATA_PATH+'/prices.csv', index_col='Date')\n",
    "\n",
    "# preprocess weekly data\n",
    "prices.index = pd.to_datetime(prices.index)\n",
    "lret_d = np.log(prices/prices.shift(1)).fillna(0.0) # weekly return\n",
    "prices = prices.resample('M').last()\n",
    "lret_m = np.log(prices/prices.shift(1)).fillna(0.0) # weekly return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature vector, which is speical to each time series\n",
    "# first 5: stock, gov bond, corp bond, real estate, commodity\n",
    "# last 3 : us, europe, other\n",
    "\n",
    "e1 = torch.Tensor([1,0,0,0,0,0,1,0]) # IEV, europe\n",
    "e2 = torch.Tensor([1,0,0,0,0,0,0,1]) # EEM, em. market\n",
    "e3 = torch.Tensor([0,0,1,0,0,1,0,0]) # AGG, us corp bonds\n",
    "e4 = torch.Tensor([0,1,0,0,0,1,0,0]) # IEF, us gov bonds\n",
    "e5 = torch.Tensor([0,0,0,1,0,1,0,0]) # IYR, us real estates\n",
    "e6 = torch.Tensor([0,0,0,0,1,0,0,0]) # IAU, gold\n",
    "e7 = torch.Tensor([1,0,0,0,0,1,0,0]) # ITOT, us equities\n",
    "\n",
    "e_dict = {0: e1, 1: e2, 2: e3, 3: e4, 4: e5, 5: e6, 6: e7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO run Optuna\n",
    "#TODO add mean and covariance to existing RP strategy and do the plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Gaussian LL Loss : 46.08000183105469\n",
      "Validation MSE   : 0.010186588391661644 \n",
      "\n",
      "Epoch 11\n",
      "Gaussian LL Loss : -3.559999942779541\n",
      "Validation MSE   : 0.011457579210400581 \n",
      "\n",
      "Epoch 21\n",
      "Gaussian LL Loss : -45.540000915527344\n",
      "Validation MSE   : 0.012191014364361763 \n",
      "\n",
      "Epoch 31\n",
      "Gaussian LL Loss : -57.04999923706055\n",
      "Validation MSE   : 0.011357217095792294 \n",
      "\n",
      "Epoch 41\n",
      "Gaussian LL Loss : -49.209999084472656\n",
      "Validation MSE   : 0.008757762610912323 \n",
      "\n",
      "Epoch 51\n",
      "Gaussian LL Loss : -63.650001525878906\n",
      "Validation MSE   : 0.0055421944707632065 \n",
      "\n",
      "Epoch 61\n",
      "Gaussian LL Loss : -73.20999908447266\n",
      "Validation MSE   : 0.01150986086577177 \n",
      "\n",
      "Epoch 71\n",
      "Gaussian LL Loss : -57.25\n",
      "Validation MSE   : 0.009251376613974571 \n",
      "\n",
      "Epoch 81\n",
      "Gaussian LL Loss : -56.13999938964844\n",
      "Validation MSE   : 0.015335332602262497 \n",
      "\n",
      "Epoch 91\n",
      "Gaussian LL Loss : -65.9800033569336\n",
      "Validation MSE   : 0.012301173992455006 \n",
      "\n",
      "Epoch 101\n",
      "Gaussian LL Loss : -65.0999984741211\n",
      "Validation MSE   : 0.012142322957515717 \n",
      "\n",
      "Epoch 111\n",
      "Gaussian LL Loss : -56.31999969482422\n",
      "Validation MSE   : 0.013600867241621017 \n",
      "\n",
      "Epoch 121\n",
      "Gaussian LL Loss : -70.44999694824219\n",
      "Validation MSE   : 0.010643440298736095 \n",
      "\n",
      "Epoch 131\n",
      "Gaussian LL Loss : -71.2300033569336\n",
      "Validation MSE   : 0.010200034826993942 \n",
      "\n",
      "Epoch 141\n",
      "Gaussian LL Loss : -58.09000015258789\n",
      "Validation MSE   : 0.008524932898581028 \n",
      "\n",
      "Epoch 151\n",
      "Gaussian LL Loss : -81.5999984741211\n",
      "Validation MSE   : 0.007632607128471136 \n",
      "\n",
      "Epoch 161\n",
      "Gaussian LL Loss : -68.91999816894531\n",
      "Validation MSE   : 0.016706788912415504 \n",
      "\n",
      "Epoch 171\n",
      "Gaussian LL Loss : -70.41000366210938\n",
      "Validation MSE   : 0.00854889489710331 \n",
      "\n",
      "Epoch 181\n",
      "Gaussian LL Loss : -76.77999877929688\n",
      "Validation MSE   : 0.009290838614106178 \n",
      "\n",
      "Epoch 191\n",
      "Gaussian LL Loss : -60.220001220703125\n",
      "Validation MSE   : 0.01134931668639183 \n",
      "\n",
      "Epoch 201\n",
      "Gaussian LL Loss : -50.970001220703125\n",
      "Validation MSE   : 0.01090196706354618 \n",
      "\n",
      "Epoch 211\n",
      "Gaussian LL Loss : -65.02999877929688\n",
      "Validation MSE   : 0.008122631348669529 \n",
      "\n",
      "Epoch 221\n",
      "Gaussian LL Loss : -66.1500015258789\n",
      "Validation MSE   : 0.01796647347509861 \n",
      "\n",
      "Epoch 231\n",
      "Gaussian LL Loss : -61.68000030517578\n",
      "Validation MSE   : 0.007902869023382664 \n",
      "\n",
      "Epoch 241\n",
      "Gaussian LL Loss : -58.66999816894531\n",
      "Validation MSE   : 0.012075649574398994 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the GP-Copula model and initialize the weight\n",
    "model = GPAutoregressiveRNN(input_size=1, hidden_size=4, num_layers=2, rank_size=4, \n",
    "                             batch_size=3, num_asset=7, dropout=0.05, features=e_dict)\n",
    "model.init_weight()\n",
    "\n",
    "# set up the loss function and optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay= 1e-5)\n",
    "num_epochs = 250\n",
    "\n",
    "# convert the data to torch tensor\n",
    "split_idx = int(lret_m.shape[0] * 0.7)\n",
    "Z_tr, Z_te = torch.Tensor(lret_m.iloc[:split_idx].values), torch.Tensor(lret_m.iloc[split_idx:].values)\n",
    "\n",
    "# transform \n",
    "X_tr, cdfs = transform(Z_tr)\n",
    "X_te, _    = transform(Z_te, cdfs)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # randomly sample sequence index for training\n",
    "    sampler = train_idx_sampler(tr_idx=Z_tr.size(0), context_len=12, prediction_len=1, num_samples=50)\n",
    "    losses = []\n",
    "    losses_valid = torch.Tensor([0.0])\n",
    "\n",
    "    for tr_idx, te_idx in sampler: # for each sequence sample\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # run the model\n",
    "        mu_t, cov_t, batch_idx = model(Z_tr[tr_idx])\n",
    "        x = X_tr[tr_idx][:,batch_idx]\n",
    "\n",
    "        # gaussian log-likelihood loss\n",
    "        loss = gaussian_loss(x, mu_t, cov_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # append loss of each batch\n",
    "        losses.append(loss.detach())\n",
    "\n",
    "        # prediction step\n",
    "        mu_pred, cov_pred, _ = model(Z_tr[te_idx], pred=True)\n",
    "        Z_tr_hat = inv_transform(mu_pred.detach(), cdfs)\n",
    "\n",
    "        losses_valid = losses_valid + (Z_tr_hat[0]- Z_tr[te_idx]).pow(2).sum()\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}') \n",
    "        print(f'Gaussian LL Loss : {np.round(np.mean(losses),2)}')\n",
    "        print(f'Validation MSE   : {losses_valid[0]/50} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fcdd98ffdc43e19459281bdd6dd2d99dc416cd8bc0df76bf27c994662bebc1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
