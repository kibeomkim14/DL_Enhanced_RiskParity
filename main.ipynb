{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "macro_data = pd.read_csv('/Users/mac/Desktop/PycharmProjects/TAADL/DATA/macroeconomic.csv', index_col='Date')\n",
    "price_data = pd.read_csv('/Users/mac/Desktop/PycharmProjects/TAADL/DATA/price_volume.csv', index_col='Date')\n",
    "\n",
    "macro_data.index = pd.to_datetime(macro_data.index)\n",
    "price_data.index = pd.to_datetime(price_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find dates that both data exist\n",
    "index = pd.to_datetime(np.intersect1d(macro_data.index, price_data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate price and macro features\n",
    "feature = pd.concat([price_data.loc[index,:], macro_data.loc[index,:]], axis=1)\n",
    "feature = feature.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BNDX_ret030</th>\n",
       "      <th>BNDX_ret060</th>\n",
       "      <th>BNDX_ret130</th>\n",
       "      <th>BNDX_ret260</th>\n",
       "      <th>BND_ret030</th>\n",
       "      <th>BND_ret060</th>\n",
       "      <th>BND_ret130</th>\n",
       "      <th>BND_ret260</th>\n",
       "      <th>VGK_ret030</th>\n",
       "      <th>VGK_ret060</th>\n",
       "      <th>...</th>\n",
       "      <th>Oil</th>\n",
       "      <th>SP500</th>\n",
       "      <th>US_Pay</th>\n",
       "      <th>US_GDP</th>\n",
       "      <th>US_CPI</th>\n",
       "      <th>10Y-2YSpread</th>\n",
       "      <th>2Y-3MSpread</th>\n",
       "      <th>Cop/Gold</th>\n",
       "      <th>SP500/DJIA</th>\n",
       "      <th>SP500/Rus2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-18</th>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>...</td>\n",
       "      <td>50.48</td>\n",
       "      <td>1426.37</td>\n",
       "      <td>137249.0</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>2.54065</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.113493</td>\n",
       "      <td>1.809816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-19</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>...</td>\n",
       "      <td>51.99</td>\n",
       "      <td>1430.50</td>\n",
       "      <td>137249.0</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>2.54065</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.113843</td>\n",
       "      <td>1.838382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-22</th>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.014703</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>0.052737</td>\n",
       "      <td>...</td>\n",
       "      <td>51.13</td>\n",
       "      <td>1422.95</td>\n",
       "      <td>137249.0</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>2.54065</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.114044</td>\n",
       "      <td>1.812767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-23</th>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.014417</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>0.061168</td>\n",
       "      <td>...</td>\n",
       "      <td>55.04</td>\n",
       "      <td>1427.99</td>\n",
       "      <td>137249.0</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>2.54065</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.113931</td>\n",
       "      <td>1.836430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-24</th>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.008007</td>\n",
       "      <td>0.060499</td>\n",
       "      <td>...</td>\n",
       "      <td>55.37</td>\n",
       "      <td>1440.13</td>\n",
       "      <td>137249.0</td>\n",
       "      <td>0.779358</td>\n",
       "      <td>2.54065</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.114099</td>\n",
       "      <td>1.830852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BNDX_ret030  BNDX_ret060  BNDX_ret130  BNDX_ret260  BND_ret030  \\\n",
       "2007-01-18     0.001124     0.003436     0.006991     0.014936    0.000652   \n",
       "2007-01-19     0.001094     0.003822     0.007210     0.014777    0.000790   \n",
       "2007-01-22     0.001374     0.003600     0.007369     0.014703    0.000930   \n",
       "2007-01-23     0.001580     0.003839     0.007256     0.014417    0.000907   \n",
       "2007-01-24     0.002012     0.003991     0.006842     0.014332    0.000637   \n",
       "\n",
       "            BND_ret060  BND_ret130  BND_ret260  VGK_ret030  VGK_ret060  ...  \\\n",
       "2007-01-18    0.001243    0.002075    0.006830    0.003104    0.052411  ...   \n",
       "2007-01-19    0.001187    0.001771    0.006768    0.006454    0.057328  ...   \n",
       "2007-01-22    0.001348    0.001741    0.007020   -0.006114    0.052737  ...   \n",
       "2007-01-23    0.001375    0.001701    0.007063    0.006741    0.061168  ...   \n",
       "2007-01-24    0.001256    0.001589    0.006941    0.008007    0.060499  ...   \n",
       "\n",
       "              Oil    SP500    US_Pay    US_GDP   US_CPI  10Y-2YSpread  \\\n",
       "2007-01-18  50.48  1426.37  137249.0  0.779358  2.54065        -0.140   \n",
       "2007-01-19  51.99  1430.50  137249.0  0.779358  2.54065        -0.149   \n",
       "2007-01-22  51.13  1422.95  137249.0  0.779358  2.54065        -0.154   \n",
       "2007-01-23  55.04  1427.99  137249.0  0.779358  2.54065        -0.144   \n",
       "2007-01-24  55.37  1440.13  137249.0  0.779358  2.54065        -0.123   \n",
       "\n",
       "            2Y-3MSpread  Cop/Gold  SP500/DJIA  SP500/Rus2000  \n",
       "2007-01-18       -0.213  0.003950    0.113493       1.809816  \n",
       "2007-01-19       -0.179  0.003937    0.113843       1.838382  \n",
       "2007-01-22       -0.195  0.003978    0.114044       1.812767  \n",
       "2007-01-23       -0.160  0.003979    0.113931       1.836430  \n",
       "2007-01-24       -0.182  0.003996    0.114099       1.830852  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test data split\n",
    "tr_date = pd.to_datetime('2014-12-31')\n",
    "te_date = tr_date + timedelta(days=1)\n",
    "ii_date = feature.index[0] # initial date of feature data\n",
    "\n",
    "df_train = feature.loc[:tr_date]\n",
    "df_test = feature.loc[te_date:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = ['BNDX_ret130','BND_ret130','VGK_ret130','VNQI_ret130','VNQ_ret130','VTI_ret130','VWOB_ret130','VWO_ret130']\n",
    "target = feature.iloc[130:2004+130][target_list]\n",
    "target = target.add_prefix('targ_')\n",
    "target.index = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize the training...\n",
      "Batch No. 0, Epoch 0: Loss = 11.101202964782715\n",
      "Batch No. 0, Epoch 25: Loss = 2.121514320373535\n",
      "Batch No. 0, Epoch 50: Loss = 1.0813286304473877\n",
      "Batch No. 0, Epoch 75: Loss = 0.6220840215682983\n",
      "Batch No. 0, Epoch 100: Loss = 0.4302310347557068\n",
      "Batch No. 0, Epoch 125: Loss = 0.363066703081131\n",
      "Batch No. 0, Epoch 150: Loss = 0.2600279748439789\n",
      "Batch No. 0, Epoch 175: Loss = 0.21947035193443298\n",
      "Batch No. 0, Epoch 200: Loss = 0.18047352135181427\n",
      "Batch No. 0, Epoch 225: Loss = 0.1621735543012619\n",
      "Batch No. 0, Epoch 250: Loss = 0.14130637049674988\n",
      "Batch No. 0, Epoch 275: Loss = 0.12959496676921844\n",
      "Batch No. 0, Epoch 300: Loss = 0.11158451437950134\n",
      "Batch No. 0, Epoch 325: Loss = 0.100038081407547\n",
      "Batch No. 0, Epoch 350: Loss = 0.09595189243555069\n",
      "Batch No. 0, Epoch 375: Loss = 0.08808155357837677\n",
      "Batch No. 0, Epoch 400: Loss = 0.08327885717153549\n",
      "Batch No. 0, Epoch 425: Loss = 0.07227706909179688\n",
      "Batch No. 0, Epoch 450: Loss = 0.06907312572002411\n",
      "Batch No. 0, Epoch 475: Loss = 0.06260787695646286\n",
      "Batch No. 1, Epoch 0: Loss = 0.07191255688667297\n",
      "Batch No. 1, Epoch 25: Loss = 0.06623958796262741\n",
      "Batch No. 2, Epoch 0: Loss = 0.07391335070133209\n",
      "Batch No. 2, Epoch 25: Loss = 0.06490034610033035\n",
      "Batch No. 3, Epoch 0: Loss = 0.07353084534406662\n",
      "Batch No. 3, Epoch 25: Loss = 0.0680331215262413\n",
      "Batch No. 4, Epoch 0: Loss = 0.07288713753223419\n",
      "Batch No. 4, Epoch 25: Loss = 0.06599224358797073\n",
      "Batch No. 5, Epoch 0: Loss = 0.060165200382471085\n",
      "Batch No. 5, Epoch 25: Loss = 0.057725679129362106\n",
      "Batch No. 6, Epoch 0: Loss = 0.0579245500266552\n",
      "Batch No. 6, Epoch 25: Loss = 0.05124573037028313\n",
      "Batch No. 7, Epoch 0: Loss = 0.053914930671453476\n",
      "Batch No. 7, Epoch 25: Loss = 0.04807313159108162\n",
      "Batch No. 8, Epoch 0: Loss = 0.05142764374613762\n",
      "Batch No. 8, Epoch 25: Loss = 0.046444837003946304\n",
      "Batch No. 9, Epoch 0: Loss = 0.047436416149139404\n",
      "Batch No. 9, Epoch 25: Loss = 0.045584503561258316\n",
      "Batch No. 10, Epoch 0: Loss = 0.045040663331747055\n",
      "Batch No. 10, Epoch 25: Loss = 0.03972608223557472\n",
      "Batch No. 11, Epoch 0: Loss = 0.03994234278798103\n",
      "Batch No. 11, Epoch 25: Loss = 0.03767500817775726\n",
      "Batch No. 12, Epoch 0: Loss = 0.03522423282265663\n",
      "Batch No. 12, Epoch 25: Loss = 0.032753899693489075\n",
      "Batch No. 13, Epoch 0: Loss = 0.032945577055215836\n",
      "Batch No. 13, Epoch 25: Loss = 0.030859151855111122\n",
      "Batch No. 14, Epoch 0: Loss = 0.029437730088829994\n",
      "Batch No. 14, Epoch 25: Loss = 0.028192812576889992\n",
      "Batch No. 15, Epoch 0: Loss = 0.02759682945907116\n",
      "Batch No. 15, Epoch 25: Loss = 0.025961972773075104\n",
      "Batch No. 16, Epoch 0: Loss = 0.02464195527136326\n",
      "Batch No. 16, Epoch 25: Loss = 0.025271901860833168\n",
      "Batch No. 17, Epoch 0: Loss = 0.024460507556796074\n",
      "Batch No. 17, Epoch 25: Loss = 0.02420976385474205\n",
      "Batch No. 18, Epoch 0: Loss = 0.02285039611160755\n",
      "Batch No. 18, Epoch 25: Loss = 0.02197209745645523\n",
      "Batch No. 19, Epoch 0: Loss = 0.02108704298734665\n",
      "Batch No. 19, Epoch 25: Loss = 0.021133417263627052\n",
      "Batch No. 20, Epoch 0: Loss = 0.020658722147345543\n",
      "Batch No. 20, Epoch 25: Loss = 0.019925447180867195\n",
      "Batch No. 21, Epoch 0: Loss = 0.019145542755723\n",
      "Batch No. 21, Epoch 25: Loss = 0.019579770043492317\n",
      "Batch No. 22, Epoch 0: Loss = 0.01859874650835991\n",
      "Batch No. 22, Epoch 25: Loss = 0.018435288220643997\n",
      "Batch No. 23, Epoch 0: Loss = 0.01724952459335327\n",
      "Batch No. 23, Epoch 25: Loss = 0.017372269183397293\n",
      "Batch No. 24, Epoch 0: Loss = 0.016675639897584915\n",
      "Batch No. 24, Epoch 25: Loss = 0.01625094562768936\n",
      "Batch No. 25, Epoch 0: Loss = 0.01611512154340744\n",
      "Batch No. 25, Epoch 25: Loss = 0.016136938706040382\n",
      "Batch No. 26, Epoch 0: Loss = 0.01531868614256382\n",
      "Batch No. 26, Epoch 25: Loss = 0.014895203523337841\n",
      "Batch No. 27, Epoch 0: Loss = 0.015567490831017494\n",
      "Batch No. 27, Epoch 25: Loss = 0.014818552881479263\n",
      "Batch No. 28, Epoch 0: Loss = 0.014634027145802975\n",
      "Batch No. 28, Epoch 25: Loss = 0.014160215854644775\n",
      "Batch No. 29, Epoch 0: Loss = 0.013943630270659924\n",
      "Batch No. 29, Epoch 25: Loss = 0.013751331716775894\n",
      "Batch No. 30, Epoch 0: Loss = 0.013773814775049686\n",
      "Batch No. 30, Epoch 25: Loss = 0.013833996839821339\n",
      "Batch No. 31, Epoch 0: Loss = 0.012710240669548512\n",
      "Batch No. 31, Epoch 25: Loss = 0.01269794162362814\n",
      "Batch No. 32, Epoch 0: Loss = 0.01282197330147028\n",
      "Batch No. 32, Epoch 25: Loss = 0.012157541699707508\n",
      "Batch No. 33, Epoch 0: Loss = 0.012332536280155182\n",
      "Batch No. 33, Epoch 25: Loss = 0.011784262955188751\n",
      "Batch No. 34, Epoch 0: Loss = 0.01151147112250328\n",
      "Batch No. 34, Epoch 25: Loss = 0.012034701183438301\n",
      "Batch No. 35, Epoch 0: Loss = 0.011271273717284203\n",
      "Batch No. 35, Epoch 25: Loss = 0.011273747310042381\n",
      "Batch No. 36, Epoch 0: Loss = 0.011039302684366703\n",
      "Batch No. 36, Epoch 25: Loss = 0.011009806767106056\n",
      "Batch No. 37, Epoch 0: Loss = 0.010962171480059624\n",
      "Batch No. 37, Epoch 25: Loss = 0.010745649226009846\n",
      "Batch No. 38, Epoch 0: Loss = 0.011114878579974174\n",
      "Batch No. 38, Epoch 25: Loss = 0.010614566504955292\n",
      "Batch No. 39, Epoch 0: Loss = 0.01080253068357706\n",
      "Batch No. 39, Epoch 25: Loss = 0.01051307749003172\n",
      "Batch No. 40, Epoch 0: Loss = 0.010169667191803455\n",
      "Batch No. 40, Epoch 25: Loss = 0.010011079721152782\n",
      "Batch No. 41, Epoch 0: Loss = 0.009806538000702858\n",
      "Batch No. 41, Epoch 25: Loss = 0.010261200368404388\n",
      "Batch No. 42, Epoch 0: Loss = 0.009948682971298695\n",
      "Batch No. 42, Epoch 25: Loss = 0.009550043381750584\n",
      "Batch No. 43, Epoch 0: Loss = 0.009379412047564983\n",
      "Batch No. 43, Epoch 25: Loss = 0.009461713023483753\n",
      "Batch No. 44, Epoch 0: Loss = 0.009792441502213478\n",
      "Batch No. 44, Epoch 25: Loss = 0.00905473344027996\n",
      "Batch No. 45, Epoch 0: Loss = 0.008902568370103836\n",
      "Batch No. 45, Epoch 25: Loss = 0.00887034647166729\n",
      "Batch No. 46, Epoch 0: Loss = 0.008858810178935528\n",
      "Batch No. 46, Epoch 25: Loss = 0.008493187837302685\n",
      "Batch No. 47, Epoch 0: Loss = 0.008676876313984394\n",
      "Batch No. 47, Epoch 25: Loss = 0.008355994708836079\n",
      "Batch No. 48, Epoch 0: Loss = 0.008311336860060692\n",
      "Batch No. 48, Epoch 25: Loss = 0.008120547980070114\n",
      "Batch No. 49, Epoch 0: Loss = 0.007995473220944405\n",
      "Batch No. 49, Epoch 25: Loss = 0.008087152615189552\n",
      "Batch No. 50, Epoch 0: Loss = 0.0077122910879552364\n",
      "Batch No. 50, Epoch 25: Loss = 0.008042731322348118\n"
     ]
    }
   ],
   "source": [
    "from network import MLP\n",
    "\n",
    "UPDATE_FREQ = 30\n",
    "BATCH_SIZE  = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# call the network for training\n",
    "model = MLP()\n",
    "model.weight_init()\n",
    "\n",
    "# set up the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "\n",
    "idx_list = [i for i in range(500,df_train.shape[0],UPDATE_FREQ)] # window lengths list\n",
    "num_windows = len(idx_list)\n",
    "\n",
    "for pointer in range(num_windows):\n",
    "    if pointer == 0: # epoch calculator\n",
    "        print('Initialize the training...')\n",
    "        epochs = 500\n",
    "    else:\n",
    "        epochs = 50\n",
    "\n",
    "    tr_idx = idx_list[pointer] \n",
    "    # take out training features and lables\n",
    "    X, y = df_train.iloc[:tr_idx], target.iloc[:tr_idx]\n",
    "\n",
    "    # normalize input X \n",
    "    mu, std = X.mean(axis=0), X.std(axis=0)\n",
    "    X = X.sub(mu).div(std)\n",
    "\n",
    "    # convert to tensor\n",
    "    X, y = torch.Tensor(X.values), torch.Tensor(y.values)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get prediction and its loss with gradients\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch%25 == 0) or (epoch == epochs):\n",
    "            print(f'Batch No. {pointer}, Epoch {epoch}: Loss = {loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0711, -0.0727,  0.0409,  ..., -0.0629,  0.0094,  0.1325],\n",
       "        [ 0.0150, -0.0010,  0.0265,  ...,  0.0288,  0.0060,  0.1315],\n",
       "        [-0.0663,  0.0298,  0.1447,  ...,  0.0923,  0.0358,  0.2894],\n",
       "        ...,\n",
       "        [-0.1254,  0.0381,  0.0391,  ...,  0.0706,  0.0636,  0.0369],\n",
       "        [ 0.0250,  0.0815,  0.0964,  ..., -0.0728,  0.1063,  0.2501],\n",
       "        [-0.0431,  0.0029,  0.2910,  ...,  0.0697,  0.1465,  0.5761]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2013e-02,  4.6438e-02,  1.2144e-01,  ...,  9.2975e-02,\n",
       "         -7.5194e-03,  3.1306e-01],\n",
       "        [-1.9703e-02, -2.4527e-02,  7.3693e-02,  ...,  5.2306e-02,\n",
       "         -1.6744e-02,  1.6092e-01],\n",
       "        [ 9.0374e-02, -5.0003e-03,  8.6113e-02,  ..., -3.7595e-02,\n",
       "          6.6569e-02,  1.8225e-01],\n",
       "        ...,\n",
       "        [-1.5495e-02, -2.5992e-04,  1.5170e-01,  ...,  8.6246e-02,\n",
       "          6.6618e-02,  2.7597e-01],\n",
       "        [-3.0425e-02,  8.7637e-03,  7.6034e-02,  ...,  3.1262e-02,\n",
       "          2.5262e-02,  4.2205e-02],\n",
       "        [ 7.6047e-02,  8.5301e-02,  1.3959e-01,  ...,  8.6441e-02,\n",
       "          2.0859e-01,  2.4312e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model uncertainty\n",
    "SIMULATION_ROUND = 1000\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
